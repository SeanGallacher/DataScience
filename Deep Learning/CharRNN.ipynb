{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Z88Y4RLJag"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cksgAH12XRjV"
      },
      "source": [
        "# Lab 6: Sequence-to-sequence models\n",
        "\n",
        "### Description:\n",
        "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
        "\n",
        "### Deliverable:\n",
        "- Fill in the code for the RNN (using PyTorch's built-in GRU).\n",
        "- Fill in the training loop\n",
        "- Fill in the evaluation loop. In this loop, rather than using a validation set, you will sample text from the RNN.\n",
        "- Implement your own GRU cell.\n",
        "- Train your RNN on a new domain of text (Star Wars, political speeches, etc. - have fun!)\n",
        "\n",
        "### Grading Standards:\n",
        "- 20% Implementation the RNN\n",
        "- 20% Implementation training loop\n",
        "- 20% Implementation of evaluation loop\n",
        "- 20% Implementation of your own GRU cell\n",
        "- 20% Training of your RNN on a domain of your choice\n",
        "\n",
        "### Tips:\n",
        "- Read through all the helper functions, run them, and make sure you understand what they are doing\n",
        "- At each stage, ask yourself: What should the dimensions of this tensor be? Should its data type be float or int? (int is called `long` in PyTorch)\n",
        "- Don't apply a softmax inside the RNN if you are using an nn.CrossEntropyLoss (this module already applies a softmax to its input).\n",
        "\n",
        "### Example Output:\n",
        "An example of my final samples are shown below (more detail in the\n",
        "final section of this writeup), after 150 passes through the data.\n",
        "Please generate about 15 samples for each dataset.\n",
        "\n",
        "<code>\n",
        "And ifte thin forgision forward thene over up to a fear not your\n",
        "And freitions, which is great God. Behold these are the loss sub\n",
        "And ache with the Lord hath bloes, which was done to the holy Gr\n",
        "And appeicis arm vinimonahites strong in name, to doth piseling \n",
        "And miniquithers these words, he commanded order not; neither sa\n",
        "And min for many would happine even to the earth, to said unto m\n",
        "And mie first be traditions? Behold, you, because it was a sound\n",
        "And from tike ended the Lamanites had administered, and I say bi\n",
        "</code>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2i_QpSsWG4c"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0: Readings, data loading, and high level training\n",
        "\n",
        "---\n",
        "\n",
        "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
        "\n",
        "* Read the following\n",
        "\n",
        "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) (Take note that you will not be implementing the encoder part of this tutorial.)\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7bdZWxvJrsx",
        "outputId": "66a7ecb7-7fce-4516-888f-776143d78b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-10 18:17:14--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 52.7.218.200, 18.214.211.171, 3.221.126.233, ...\n",
            "Connecting to piazza.com (piazza.com)|52.7.218.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2022-02-10 18:17:14--  https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving cdn-uploads.piazza.com (cdn-uploads.piazza.com)... 99.84.110.25, 99.84.110.97, 99.84.110.19, ...\n",
            "Connecting to cdn-uploads.piazza.com (cdn-uploads.piazza.com)|99.84.110.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "\r./text_files.tar.gz   0%[                    ]       0  --.-KB/s               \r./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-02-10 18:17:14 (80.7 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 18.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "file_len = 2579888\n"
          ]
        }
      ],
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        " \n",
        "import pdb\n",
        " \n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxBeKeNjJ0NQ",
        "outputId": "bca60642-6d27-4ffc-b316-53c25bb53945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " he set foot upon the far bank of Silverlode a strange \n",
            "feeling had come upon him, and it deepened as he walked on into the Naith: \n",
            "it seemed to him that he had stepped over a bridge of time into a cor\n"
          ]
        }
      ],
      "source": [
        "chunk_len = 200\n",
        " \n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "  \n",
        "print(random_chunk())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On0_WitWJ99e",
        "outputId": "1129a655-92ea-4eae-c81b-4bb2514f5dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return tensor\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYJPTLcaYmfI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Creating your own GRU cell \n",
        "\n",
        "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
        "\n",
        "---\n",
        "\n",
        "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
        "\n",
        "Please try not to look at the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
        "\n",
        "**TODO:**\n",
        "* Create a custom GRU cell\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aavAv50ZKQ-F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "    \n",
        "    self.__dict__.update(locals())\n",
        "   \n",
        "    self.W = nn.ModuleList()\n",
        "    self.B = nn.ParameterList()\n",
        "\n",
        "    for l in range(num_layers):\n",
        "      self.W_ir, self.W_iz, self.W_in, = nn.Linear(self.input_size, self.hidden_size),nn.Linear(self.input_size, self.hidden_size),nn.Linear(self.input_size, self.hidden_size)\n",
        "      self.W_hz, self.W_hn, self.W_hr = nn.Linear(self.hidden_size, self.hidden_size),nn.Linear(self.hidden_size, self.hidden_size),nn.Linear(self.hidden_size, self.hidden_size)\n",
        "      for w in [self.W_ir, self.W_hr, self.W_iz,self.W_hz, self.W_in, self.W_hn]:\n",
        "        self.W.append(w)\n",
        "      \n",
        "      self.b_ir =  self.b_iz =  self.b_in  =  self.b_hr =  self.b_hz =   self.b_hn = Parameter(torch.Tensor(1, \n",
        "                            self.hidden_size))\n",
        "      for b in [self.b_ir, self.b_iz, self.b_in,  self.b_hr,  self.b_hz, self.b_hn]:\n",
        "        torch.nn.init.xavier_uniform_(b, gain=1.0)\n",
        "\n",
        "    self.s1 = nn.Sigmoid()\n",
        "    self.s2 = nn.Sigmoid()\n",
        "    self.t1 = nn.Tanh()\n",
        "\n",
        "  def forward(self, inputs, hidden):\n",
        "    hidden_new = []\n",
        "    for i, l in enumerate(range(self.num_layers)):\n",
        "      #print(torch.matmul(inputs, self.W_ir).shape, torch.matmul(hidden, self.W_hr ).shape, self.b_hr.shape)\n",
        "      r_t = self.s1(self.W[0+l*i](inputs) + self.W[1+l*i](hidden[i]) + self.b_hr)\n",
        "      z_t = self.s2(self.W[2+l*i](inputs) + self.W[3+l*i](hidden[i])   + self.b_hz)\n",
        "      n_t = self.t1(self.W[4+l*i](inputs) + torch.mul( r_t, self.W[5+l*i](hidden[i]) + self.b_hn) + self.b_hn)\n",
        "      h_t = torch.mul(1 - z_t, n_t) + torch.mul(z_t, hidden[i])\n",
        "      hidden_new.append(h_t)\n",
        "    hidden = torch.cat(hidden_new)\n",
        "    outputs = h_t\n",
        "    \n",
        "    return outputs, hidden\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtXdX-B_WiAY"
      },
      "source": [
        "---\n",
        "\n",
        "##  Part 1: Building a sequence to sequence model\n",
        "\n",
        "---\n",
        "\n",
        "Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n",
        "\n",
        "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "* Create an RNN class that extends from nn.Module.\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "d6tNdEnzWj5F"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    # more stuff here...\n",
        "    self.GRU = GRU(input_size, hidden_size, n_layers)\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, input_char, hidden):\n",
        "    # by reviewing the documentation, construct a forward function that properly uses the output of the GRU\n",
        "    output = self.embedding(input_char).view(1, 1, -1)\n",
        "    output = F.relu(output)\n",
        "\n",
        "    out_decoded, hidden = self.GRU(output, hidden)\n",
        "    \n",
        "    out_decoded = self.out(out_decoded[0])\n",
        "\n",
        "    return out_decoded, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hrhXghEPKD-5"
      },
      "outputs": [],
      "source": [
        "def random_training_set():    \n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpiGObbBX0Mr"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "We now want to be able to train our network, and sample text after training.\n",
        "\n",
        "This function outlines how training a sequence style network goes. \n",
        "\n",
        "**TODO:**\n",
        "* Fill in the pieces.\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "2ALC3Pf8Kbsi"
      },
      "outputs": [],
      "source": [
        "# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables\n",
        "\n",
        "n_hidden = 128\n",
        "\n",
        "### hello my name is ____ bob \n",
        "### coverts to tenvector tensor vector, then you model model will bedm \n",
        "### training text, Take a random section from the text \n",
        "### inp = i went to the store today\n",
        "### target =  went to the store today.\n",
        "\n",
        "input_size, hidden_size, output_size = 100,100,n_characters\n",
        "learning_rate = 0.001\n",
        "rnn = RNN(input_size, hidden_size, output_size)\n",
        "decoder_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "objective = nn.CrossEntropyLoss()\n",
        "def train(inp, target):\n",
        "  decoder_optimizer.zero_grad()\n",
        "  hidden = rnn.init_hidden()\n",
        "  loss = 0\n",
        "\n",
        "  for i in range(inp.size()[0]):\n",
        "\n",
        "      output, hidden = rnn(inp[i], hidden)\n",
        "      loss += objective(output, target[i].unsqueeze(0))\n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "  return  loss.item()\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "-7Q8ceUhv45a"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN06NUu3YRlz"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
        "\n",
        "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n",
        "\n",
        "**TODO:**\n",
        "* Fill out the evaluate function to generate text frome a primed string\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "B-bp-OZ1KjNh"
      },
      "outputs": [],
      "source": [
        "def sample_outputs(output, temperature):\n",
        "    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n",
        "    return torch.multinomial(torch.exp(output / temperature), 1)\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=.5):\n",
        "  ## initialize hidden state, initialize other useful variables\n",
        "    # your code here\n",
        "  ## /\n",
        "  hidden = rnn.init_hidden()\n",
        "  input = char_tensor(prime_str)\n",
        "  for i in range(len(input[:-1])):\n",
        "      output, hidden = rnn(input[i], hidden)\n",
        "\n",
        "  input = input[-1]\n",
        "  for i in range(predict_len):\n",
        "      output, hidden = rnn(input, hidden)\n",
        "      new_char = all_characters[sample_outputs(output,temperature).item()]\n",
        "      prime_str+= new_char\n",
        "      input = char_tensor(new_char)\n",
        "      #print(prime_str)\n",
        "\n",
        "  return prime_str\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du4AGA8PcFEW"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: (Create a GRU cell, requirements above)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFS2bpHSZEU6"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Part 5: Run it and generate some text!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**TODO:** \n",
        "* Create some cool output\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs. These are the results, along with the prime string:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-nXFeCmdKodw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "n_epochs = 5000\n",
        "print_every = 20\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "\n",
        " \n",
        "#decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xKfozqw-6eqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840bcdac-5f78-467c-bcc9-7a4b8c61c349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.374957323074341 (20 0%) 625.6779]\n",
            "Wh   i alhnne eat ae e ennnnJ  o e s ine e   etlr deia   nc dh n hdi en o thae e ihlon en i  ea n   hh \n",
            "\n",
            "[8.464582443237305 (40 0%) 620.8953]\n",
            "Wh taoee   o . h eo ooe de  at ow s s sane tre  saf e ttll   t n ao  t thes  uam t  waa i o s n  t ete \n",
            "\n",
            "[12.24028205871582 (60 1%) 568.5573]\n",
            "Wh t t tte e   he d t hr ata ahe th n he ae  ha tn.  \n",
            "f ar he t  e thstloni h mtt it th mote heo t t t \n",
            "\n",
            "[16.061116695404053 (80 1%) 549.4240]\n",
            "Wharth s  al orai tohe he thre \n",
            "ao we tie  the s at eth te twe i h ou no thn the te the tline to f he  \n",
            "\n",
            "[19.87338876724243 (100 2%) 539.5125]\n",
            "When te uanasrn tre thent w nao raea at aso \n",
            "af thehe he tanid and nd are noe d, se gin ind sas ans t  \n",
            "\n",
            "[23.855522632598877 (120 2%) 520.1741]\n",
            "Whe fe the the the \n",
            "o the gol de ad toat hero ano t hass ing the the the ait hhe me tthe oine foe the  \n",
            "\n",
            "[27.724486589431763 (140 2%) 496.8302]\n",
            "Whed wed rinwind awer ante the the h theet the ans anon the \n",
            "and ant here the the cee ad fo the a\n",
            "ewe  \n",
            "\n",
            "[31.622120141983032 (160 3%) 521.6121]\n",
            "Whe \n",
            "ans an on athe he sale \n",
            "ceand son the the her the \n",
            "o thee no the anat  at the the sot an oo thare \n",
            "\n",
            "[35.599549770355225 (180 3%) 482.0952]\n",
            "Whe \n",
            "out band \n",
            "ind she wo the nore he the it hea the ad cound and aen the hand the berd tlothas \n",
            "\n",
            "of w \n",
            "\n",
            "[39.484058141708374 (200 4%) 487.8773]\n",
            "Whed are the foule the the innd the the the te \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "art tore the ye the the ires \n",
            "sare the the ang he  \n",
            "\n",
            "[44.11444115638733 (220 4%) 502.9070]\n",
            "Whe han hadn hat the the the the ve wanda t waw the an meres the ang was arm othe she tou wos tore an  \n",
            "\n",
            "[48.22952580451965 (240 4%) 462.6115]\n",
            "Whe we mo be on an sam the he \n",
            "\n",
            "and I thin the the cear ind Nopere \n",
            "we the \n",
            "\n",
            "\n",
            "war wall ne \n",
            "come we hon \n",
            "\n",
            "[52.102569341659546 (260 5%) 473.7077]\n",
            "Whe \n",
            "\n",
            "'and in. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "the bere \n",
            "\n",
            "\n",
            "\n",
            "I \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ere hand be warll and Sor gof the ome he shel the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'he a \n",
            "\n",
            "[56.73362970352173 (280 5%) 465.5563]\n",
            "Whe sind ar das and the the the the the sous as and the \n",
            "\n",
            "ind ned theing he bo ound \n",
            "\n",
            "\n",
            "' \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "an the \n",
            "\n",
            "[60.66122126579285 (300 6%) 476.1986]\n",
            "Whe \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "le \n",
            "lald and the of ho the \n",
            "in and the \n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[64.80299782752991 (320 6%) 488.9632]\n",
            "Whe in are and and the and and the at ind oun the be dars the wall meand he mend mome sere the the the \n",
            "\n",
            "[68.86069893836975 (340 6%) 426.6682]\n",
            "Whars are nound sig the ghen ind and the \n",
            "\n",
            "' \n",
            "\n",
            "\n",
            "I the and an of has the ong in he sout wand ang were t \n",
            "\n",
            "[72.75807619094849 (360 7%) 437.7406]\n",
            "Whe tho mis and bere the here an jouy sont the the weele \n",
            "\n",
            "'the and out the to ted the the aro ha bor  \n",
            "\n",
            "[76.53748297691345 (380 7%) 480.2353]\n",
            "Whe \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Forerngh hand the me the and the had he sand and the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "and of and that uren the \n",
            "\n",
            "ore \n",
            "\n",
            "[80.65325975418091 (400 8%) 442.0268]\n",
            "Wh and of of the he the fis the a fill the \n",
            "I hithe wall hand dound and of shat has hat the wis the hi \n",
            "\n",
            "[84.58359122276306 (420 8%) 431.0060]\n",
            "Wh cave the om the her he whe \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'he the sard and yor the whor the I the send the \n",
            "\n",
            "sh \n",
            "\n",
            "[88.40387845039368 (440 8%) 417.3907]\n",
            "Whe as and has the youns the and the the the so the gas in the shas me sithe cor sat and ronted the wa \n",
            "\n",
            "[92.23730564117432 (460 9%) 458.6920]\n",
            "Whe there \n",
            "lithe wand bere laro thet and the \n",
            "bered out hink wand and the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The wat his sout the  \n",
            "\n",
            "[96.30057191848755 (480 9%) 412.2038]\n",
            "Whe and ing his \n",
            "\n",
            "\n",
            "moren this ware s wean the war the wand bet shand the mout beer wand the sot he san \n",
            "\n",
            "[100.26451516151428 (500 10%) 514.5463]\n",
            "Whe the \n",
            "\n",
            "'ling on thait and the the shit ang the reas and the \n",
            "'he was the \n",
            "\n",
            "\n",
            "\n",
            "'Ye \n",
            "ming or the \n",
            "\n",
            "\n",
            "\n",
            "' \n",
            "\n",
            "[104.19230008125305 (520 10%) 416.8230]\n",
            "Whing  fore \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The \n",
            "\n",
            "'I and sat the wat ssuld pout dors the the doud and the wear and the solred o \n",
            "\n",
            "[108.01869606971741 (540 10%) 451.2327]\n",
            "Wh the the \n",
            "\n",
            "\n",
            "\n",
            "I more the was wall in sas the mive sering the sore saing the bearm me has the roll ige \n",
            "\n",
            "[112.07138109207153 (560 11%) 376.2239]\n",
            "Whe he sing and and cinge the be the and \n",
            "\n",
            "'he \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'And of the the sing and and the all for sead pu \n",
            "\n",
            "[115.84724426269531 (580 11%) 419.0572]\n",
            "When that the sane the the \n",
            "\n",
            "'he the hing hape the sone the The wass is there he cound the the \n",
            "Songo  \n",
            "\n",
            "[119.63308334350586 (600 12%) 362.8481]\n",
            "Whe the weard wope to the goured the the wast a daid the stout the had the lamy the and the hing, and  \n",
            "\n",
            "[123.55342960357666 (620 12%) 441.6128]\n",
            "Wh the ream out as aren of the land he dish and the the the wiss in the wall of ther and in the wills  \n",
            "\n",
            "[127.54530549049377 (640 12%) 407.4585]\n",
            "Wh moster, and lead the bes \n",
            "\n",
            "'I and the call the wis fore and she ste cesto hand \n",
            "fiss crill \n",
            "\n",
            "'the s \n",
            "\n",
            "[131.31107664108276 (660 13%) 392.1822]\n",
            "Wher were the do he he fard was werned are wee the the the \n",
            "ill he sand I was the \n",
            "\n",
            "\n",
            "I doull co the yo \n",
            "\n",
            "[135.1830289363861 (680 13%) 391.2464]\n",
            "Wh and in the \n",
            "the the \n",
            "the \n",
            "of this stoun dord ar dout and wis at the be and be soad had fare the har \n",
            "\n",
            "[139.13399291038513 (700 14%) 417.6158]\n",
            "Wh in the lo en he was and thing sand the \n",
            "the \n",
            "the coong the candor and be mourn end the sang the his \n",
            "\n",
            "[143.14445734024048 (720 14%) 402.5216]\n",
            "Whe soun the were \n",
            "lith \n",
            "and mans of has all fere wound and the dead in haid and the fard the jand we  \n",
            "\n",
            "[146.91369605064392 (740 14%) 397.6474]\n",
            "Wh whand the \n",
            "a \n",
            "\n",
            "\n",
            "\n",
            "'Sot and sill be was hat the was sto his of a she the mand dore of \n",
            "but fill sroul \n",
            "\n",
            "[150.9118378162384 (760 15%) 452.5405]\n",
            "Wh and the for the be the \n",
            "\n",
            "'Me see his of the you the the mone bent ther \n",
            "\n",
            "wame ward a dor the lame t \n",
            "\n",
            "[154.76632976531982 (780 15%) 396.8736]\n",
            "Wh and ar wand and of the ligh as a dall the \n",
            "\n",
            "And the and \n",
            "and were and the vand po the fimang and th \n",
            "\n",
            "[158.72399401664734 (800 16%) 450.3656]\n",
            "Wh all suided of \n",
            "we has was and hand war foll in the he war the here beed the lost as a the thim sill \n",
            "\n",
            "[162.57833886146545 (820 16%) 419.3209]\n",
            "Wher mored the \n",
            "\n",
            "mow the \n",
            "the greed with the \n",
            "\n",
            "\n",
            "\n",
            "'I said the reared it the to the hand the \n",
            "cnow with  \n",
            "\n",
            "[166.4017791748047 (840 16%) 428.1832]\n",
            "Wh in sing \n",
            "of the pain the rotrows the hand in the dow \n",
            "ta hat bead in the some rith of his for the t \n",
            "\n",
            "[170.20386719703674 (860 17%) 405.5630]\n",
            "Wh for a seels in the was stor a ding taing fill \n",
            "ing at wam laid are and \n",
            "the said \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "But and land  \n",
            "\n",
            "[174.17041945457458 (880 17%) 413.4918]\n",
            "Wh the dore wring dound the was sand the \n",
            "the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'I stord will hat of he said would the \n",
            "corded \n",
            "\n",
            "[177.96150541305542 (900 18%) 391.6867]\n",
            "Wh and getost dere the the the \n",
            "said the mand for \n",
            "\n",
            "\n",
            "\n",
            "'Bot he the has shand he land in the dear the ha \n",
            "\n",
            "[181.79315447807312 (920 18%) 391.5128]\n",
            "Whe not the he seen the beter and the shound the gole it and of the sild he sare some the still bentir \n",
            "\n",
            "[185.623220205307 (940 18%) 383.9365]\n",
            "When lone the seen the rith the hought of the dark has he \n",
            "the sill the \n",
            "\n",
            "Bould in the wime the pander \n",
            "\n",
            "[189.47540855407715 (960 19%) 402.7872]\n",
            "Wh stard and \n",
            "the sing the was wive shith the \n",
            "said \n",
            "the round of the was hind of the \n",
            "wentand the see \n",
            "\n",
            "[193.52398777008057 (980 19%) 406.5211]\n",
            "Wh and they was the the rounter and in was spack dound where and wist were star the stile the begring  \n",
            "\n",
            "[197.3556890487671 (1000 20%) 343.5136]\n",
            "Wh the \n",
            "wapand the haved in the wat the last at of the the some sare walked the was the bent of the \n",
            "l \n",
            "\n",
            "[201.29103660583496 (1020 20%) 438.2260]\n",
            "Wh \n",
            "\n",
            "rear the cenet wat in the lang whis \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'It there but on in the was stole the shat the pull a \n",
            "\n",
            "[205.05281925201416 (1040 20%) 360.3507]\n",
            "Whe dore been were wall the \n",
            "\n",
            "was and las his seare and youn and and to the strow the mast and not was \n",
            "\n",
            "[208.8663272857666 (1060 21%) 364.1713]\n",
            "Wh the have the \n",
            "\n",
            "The \n",
            "courts and ever and seen the dor the pent the his it store he sill his store th \n",
            "\n",
            "[212.74216032028198 (1080 21%) 389.5844]\n",
            "Wher all the he \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'I make his stack the hard store up the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "no back thin said frod \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[216.6634840965271 (1100 22%) 340.2280]\n",
            "Wh the domped ut the ragker were we for the store were bet we \n",
            "for the last mang the kloke then in the \n",
            "\n",
            "[220.45675778388977 (1120 22%) 421.4968]\n",
            "Wher to the fark, and the \n",
            "it to he stould a spait of the grom and the \n",
            "and the \n",
            "were and looked the p \n",
            "\n",
            "[224.3819625377655 (1140 22%) 407.4204]\n",
            "Whed the said beat was comed and be in the rere the said \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'said \n",
            "shen do door far the shad trome a \n",
            "\n",
            "[228.35448575019836 (1160 23%) 350.6095]\n",
            "Wh \n",
            "therren the sited the \n",
            "\n",
            "'will of the will the kent yee the shey the mounder and the thing a from \n",
            " \n",
            "\n",
            "[232.2955310344696 (1180 23%) 364.4681]\n",
            "Wher the said fard and and a hard the wreast. I more of the good he said. There a dound the crome buth \n",
            "\n",
            "[236.08372259140015 (1200 24%) 352.6587]\n",
            "Wher the last and not ake his hand of the dong and the long the said was the said in the \n",
            "mange came a \n",
            "\n",
            "[239.90519881248474 (1220 24%) 412.8271]\n",
            "Wher wish the counther and he green be of the was he \n",
            "the conce of the was and he stere of the said hi \n",
            "\n",
            "[243.68174815177917 (1240 24%) 366.0524]\n",
            "Wh and \n",
            "the sear was loon \n",
            "the sull not cove some and the mant in the hand \n",
            "wat still stood \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[247.4327518939972 (1260 25%) 345.7856]\n",
            "Whe whear still down he shis the for hitt \n",
            "had his doonger the dare in the home the mone the more and  \n",
            "\n",
            "[251.29145503044128 (1280 25%) 373.5378]\n",
            "Whey \n",
            "hould the hist dore of the said \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'And land a see \n",
            "hill have of the hight  \n",
            "\n",
            "[255.12559509277344 (1300 26%) 345.3971]\n",
            "Wh whith the marked to the fark and be a stard a \n",
            "a garked and had there betore were hik fors, and he  \n",
            "\n",
            "[258.89925956726074 (1320 26%) 373.0584]\n",
            "Wher and \n",
            "pace a stwall bear of the the bear a far to the \n",
            "\n",
            "of were with was of the brack in the mores \n",
            "\n",
            "[262.8704056739807 (1340 26%) 345.9075]\n",
            "Wh when he \n",
            "the \n",
            "and begong the the come geen the grom and the come the cove go of the pappin the the  \n",
            "\n",
            "[266.6336097717285 (1360 27%) 383.8968]\n",
            "Wher to the shad the ringer a for a green the come to the the it will said I kent be the \n",
            "and the grou \n",
            "\n",
            "[270.4488959312439 (1380 27%) 384.2399]\n",
            "Wh the the rand still the deen and a side the store and the fare and the dead about bearms said for \n",
            "w \n",
            "\n",
            "[274.3521308898926 (1400 28%) 353.9806]\n",
            "Wh the the was bars and the \n",
            "cood the pore the some of out the peard of the a that me the the here was \n",
            "\n",
            "[278.42568159103394 (1420 28%) 352.5110]\n",
            "Wh a cack the sains of the said the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Wong for the laghing and a some they be ling one of the the m \n",
            "\n",
            "[282.2090473175049 (1440 28%) 386.5085]\n",
            "When so the was \n",
            "light before ol To be she smoll and me the hive sare in the sleen the ment \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[286.0392179489136 (1460 29%) 352.8594]\n",
            "Wh to the stard \n",
            "for the morad to he for when the laghe the said the stour the rear in in a \n",
            "and was w \n",
            "\n",
            "[289.8752989768982 (1480 29%) 348.1104]\n",
            "Whing a mandor the \n",
            "the beat of the Mono the his hear. \n",
            "\n",
            "'There have somoun there \n",
            "treen the wime the  \n",
            "\n",
            "[293.66293597221375 (1500 30%) 393.5026]\n",
            "Wh shat light as the rome the long it the was sore the mack by he dow \n",
            "the to the see said there \n",
            "read \n",
            "\n",
            "[297.7639467716217 (1520 30%) 383.0960]\n",
            "When the beat of the said \n",
            "was see where the has his blown you what he brother the \n",
            "shat in Ban the sh \n",
            "\n",
            "[301.6754491329193 (1540 30%) 333.8911]\n",
            "Wher. I begurned the winding and the \n",
            "seen the wame the \n",
            "has the rould the \n",
            "parking the dored to the c \n",
            "\n",
            "[305.4759612083435 (1560 31%) 348.5460]\n",
            "Wh and all at won the food the \n",
            "sill his \n",
            "the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'It be have and the ricken were and said I \n",
            "an \n",
            "\n",
            "[309.3242566585541 (1580 31%) 398.6189]\n",
            "Wh we his read \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "I \n",
            "\n",
            "[313.15732073783875 (1600 32%) 364.9919]\n",
            "Wher were countary the of the was here for have the \n",
            "and of the sores of the would he soon of the word \n",
            "\n",
            "[317.0073993206024 (1620 32%) 388.4380]\n",
            "Wher sing belies of the fack for the was hould shad the should will he shall all with he \n",
            "and we \n",
            "shan \n",
            "\n",
            "[320.8567593097687 (1640 32%) 392.2368]\n",
            "Wher his has mowing of the \n",
            "great \n",
            "the blong to some to the \n",
            "store the ricked that is now and lan the  \n",
            "\n",
            "[324.8017315864563 (1660 33%) 324.2381]\n",
            "Wh the markn to thound waster the my fark the \n",
            "of the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "and the \n",
            "say, and the not from and in the  \n",
            "\n",
            "[328.7214729785919 (1680 33%) 366.3389]\n",
            "Wher the will the brothers and the \n",
            "the was store to the down the come rope the great his more there t \n",
            "\n",
            "[332.78534269332886 (1700 34%) 373.1007]\n",
            "Whing and the for would ace with the sing is the mack and the ridon the great was and was and stard th \n",
            "\n",
            "[336.59239745140076 (1720 34%) 352.7115]\n",
            "Wh a shouring and the sill and the vast aftle have are was me shound of to the not the rome was in the \n",
            "\n",
            "[340.4287610054016 (1740 34%) 389.5677]\n",
            "Whe treets and so was beard to a smang was stole bett was at the \n",
            "goods and he seew the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'Wree be  \n",
            "\n",
            "[344.4607207775116 (1760 35%) 359.8536]\n",
            "Wher the his end the sent were they the some the some with the done of the shours come the Elft the hi \n",
            "\n",
            "[348.3171765804291 (1780 35%) 393.4598]\n",
            "When of the rould see you said Gare was firear leat have seep and the many sill of the he surt do said \n",
            "\n",
            "[352.151175737381 (1800 36%) 341.2581]\n",
            "Wher the storned the Ell the dark in the dound and see was the could be green be and see a stile the s \n",
            "\n",
            "[356.01866149902344 (1820 36%) 371.5278]\n",
            "Wh the may sith in the men was look be of the did in a half the was say in when the pat in the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[359.84884548187256 (1840 36%) 396.0405]\n",
            "Wher the ore was store suon to are seen the \n",
            "seam and they hove the seeped and the store they \n",
            "and to  \n",
            "\n",
            "[363.7041208744049 (1860 37%) 355.1891]\n",
            "Whors of the store the Dor the prome was \n",
            "the gread the \n",
            "and whough the said I with of the will the Ca \n",
            "\n",
            "[367.73100304603577 (1880 37%) 368.7641]\n",
            "Wher sted of \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "the it sould and the rear and a lief the rings and not sost wa \n",
            "\n",
            "[371.4997720718384 (1900 38%) 345.2895]\n",
            "Wher \n",
            "plale it \n",
            "shem and to him the lave the like stade to the store door \n",
            "as the was the for the some \n",
            "\n",
            "[375.24422883987427 (1920 38%) 329.0486]\n",
            "Wher of a said Gids the mise be seen in the seen the for the said Sam that in the will a they fell of  \n",
            "\n",
            "[379.1236605644226 (1940 38%) 356.8264]\n",
            "Wh the far and the were was was the said the beared the land and his now the with or the \n",
            "silled \n",
            "hand \n",
            "\n",
            "[382.8676874637604 (1960 39%) 349.4025]\n",
            "Wher the \n",
            "buid the would to said \n",
            "him the whill the his shill the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'the with the was ho \n",
            "\n",
            "[386.74035596847534 (1980 39%) 408.7521]\n",
            "Wh the was to he to the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "far end the st \n",
            "\n",
            "[390.64124178886414 (2000 40%) 329.5867]\n",
            "Wher we mand. 'We well were the preesed in the for stare the ling and cound the light and wish now the \n",
            "\n",
            "[394.6102628707886 (2020 40%) 366.8599]\n",
            "Wh on the now \n",
            "seelmed the hise the was he deat of the migut the did some the slown \n",
            "and the thing out \n",
            "\n",
            "[398.4553756713867 (2040 40%) 384.9236]\n",
            "Wher and the came in the his now \n",
            "do dood and the coomed the see and the great the Sam the slooked on  \n",
            "\n",
            "[402.31101870536804 (2060 41%) 362.1845]\n",
            "Whing and the shander to the prone the \n",
            "could the said Gonder of the had his reat his \n",
            "him the door an \n",
            "\n",
            "[406.3963997364044 (2080 41%) 393.3131]\n",
            "Wh the pread the dees and stime have beary the Nor with the Ridond the bent land in the like the Right \n",
            "\n",
            "[410.41819882392883 (2100 42%) 325.2372]\n",
            "Wher \n",
            "dain as many the wond. 'I he and as the right of the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "him the benter and the far fire  \n",
            "\n",
            "[414.1835436820984 (2120 42%) 345.7423]\n",
            "Wher had of the \n",
            "in the trees and the silled I for of the from the Ritsers of the see is was the miss  \n",
            "\n",
            "[418.0361452102661 (2140 42%) 351.5373]\n",
            "Wh a said and the whould his laid and past crise the Elan the freed some all the \n",
            "sile. There strees a \n",
            "\n",
            "[421.9131624698639 (2160 43%) 310.4660]\n",
            "Wher the said Gander the looked and the came and \n",
            "we \n",
            "the \n",
            "stear, and and the was a far all that we \n",
            "a \n",
            "\n",
            "[425.65892815589905 (2180 43%) 412.3295]\n",
            "Wher \n",
            "that a was beas laid of the Elver ment the Elves a great the rise of the Sam the will bether, an \n",
            "\n",
            "[429.49622201919556 (2200 44%) 364.8612]\n",
            "Wh as the \n",
            "and the Cast been to the like the sees of the of the said the \n",
            "\n",
            "\n",
            "\n",
            "in we come the great he s \n",
            "\n",
            "[433.23224353790283 (2220 44%) 345.4465]\n",
            "Wh \n",
            "they before to the was some of the \n",
            "looks the good all and be is and was looks of for when and and \n",
            "\n",
            "[437.11239194869995 (2240 44%) 348.6213]\n",
            "Wher in the said Gander at here his string the cime the mallow were had the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sai \n",
            "\n",
            "[440.9490542411804 (2260 45%) 322.9858]\n",
            "Wher and \n",
            "he rime the past of the come to Mere of \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'he sare of the Shordered well the parsed and \n",
            "\n",
            "[444.74842047691345 (2280 45%) 348.0367]\n",
            "Wher the dord \n",
            "and the dark of the \n",
            "\n",
            "\n",
            "\n",
            "'On the paid to the words was then in the come to the mare was  \n",
            "\n",
            "[448.66453671455383 (2300 46%) 331.4809]\n",
            "Wher there he said Gandalf the good in the \n",
            "wisher and the didges it the mall and dark and he seessed  \n",
            "\n",
            "[452.4488959312439 (2320 46%) 371.0353]\n",
            "Wher to \n",
            "was \n",
            "when some of \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'I and \n",
            "seen but in the Elend was a great he said the seeps  \n",
            "\n",
            "[456.4043667316437 (2340 46%) 360.7513]\n",
            "Wh the seemed had Day they the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[460.2098367214203 (2360 47%) 327.9991]\n",
            "Wh silent of the diven the were to that at kent with of the more in the rest they black staraded he be \n",
            "\n",
            "[463.9567303657532 (2380 47%) 339.1504]\n",
            "Wher of the put of the wind they hall the good your was some we green \n",
            "his a grown the sut and \n",
            "all st \n",
            "\n",
            "[467.8863949775696 (2400 48%) 294.5021]\n",
            "Whering and the North were a smant the said, and \n",
            "the mand, and the come the creat the rays the preat  \n",
            "\n",
            "[471.71154260635376 (2420 48%) 400.2856]\n",
            "Wher the said the romed in the said the frodo the of a store wimly \n",
            "the sich for the wang of the \n",
            "was  \n",
            "\n",
            "[475.505215883255 (2440 48%) 356.9555]\n",
            "Whing of the seels good in as in the \n",
            "you were a dark be came with he so more in the greater and must  \n",
            "\n",
            "[479.359090089798 (2460 49%) 358.0706]\n",
            "Wh you the wander of the his a streen the leable and of the \n",
            "seemed to seel be got in the tull it stre \n",
            "\n",
            "[483.14233136177063 (2480 49%) 338.2260]\n",
            "Wh will the feeled. And a chand they his was would \n",
            "of the will been was made for a standing and store \n",
            "\n",
            "[487.0603666305542 (2500 50%) 368.9028]\n",
            "Wh a green ith and be \n",
            "silled looked the was hound the wall and upon a said Sam had a said Frodo was s \n",
            "\n",
            "[491.0178518295288 (2520 50%) 364.5368]\n",
            "Wher \n",
            "started to see a manged and strode and he stood. 'Is all the paite, and \n",
            "the dear the darkness o \n",
            "\n",
            "[494.79247879981995 (2540 50%) 364.0043]\n",
            "Wher and the was in the of must we went a \n",
            "mist he said Frodo,' said Gander of the pasked and down the \n",
            "\n",
            "[498.74232006073 (2560 51%) 359.6516]\n",
            "Wher that it not hearts and the \n",
            "froke the \n",
            "shane, but in the filless been it a land \n",
            "one the reat of  \n",
            "\n",
            "[502.61487340927124 (2580 51%) 417.0078]\n",
            "Wh that we could not the were spong the drodo of the had and again the \n",
            "light is and and the the word  \n",
            "\n",
            "[506.5270013809204 (2600 52%) 345.4513]\n",
            "Whing. \n",
            "\n",
            "'They dearly said then still the piter the pand the said Frodo \n",
            "and floward of the mist that  \n",
            "\n",
            "[510.3728609085083 (2620 52%) 399.6950]\n",
            "Wher the hould againg of \n",
            "the kneath the hill and the wooked there had that light was have we dark of  \n",
            "\n",
            "[514.1502623558044 (2640 52%) 348.2985]\n",
            "Wher seads the fount \n",
            "the like down the said \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "But \n",
            "list of the sink and the la \n",
            "\n",
            "[518.124011516571 (2660 53%) 345.8251]\n",
            "Whe would he seat on the the \n",
            "sear under the reat may to the was ad he was we said, and there and the  \n",
            "\n",
            "[521.9513430595398 (2680 53%) 355.5010]\n",
            "Whan were to the was the packed the stood out and \n",
            "goor the said Frodo this and the \n",
            "like he whould no \n",
            "\n",
            "[525.7367384433746 (2700 54%) 308.6654]\n",
            "Wher had a can \n",
            "ever \n",
            "and \n",
            "the \n",
            "reen have and \n",
            "the \n",
            "still house, and the know that he \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[529.6100397109985 (2720 54%) 326.4963]\n",
            "Whing of the will and for the was said foor spered that his stanged to hard him be hid and mest of sho \n",
            "\n",
            "[533.3924477100372 (2740 54%) 355.8023]\n",
            "Wher be \n",
            "said Gould wine place to the said the said the for \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "the Civer a \n",
            "all \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[537.177806854248 (2760 55%) 367.0039]\n",
            "Wher is him after some some to the streed beith the plack were shembed the bees the beldow and the low \n",
            "\n",
            "[540.9194142818451 (2780 55%) 443.1436]\n",
            "Wher the chand the must the courter can he mand the chould \n",
            "looked I hund at \n",
            "the \n",
            "could and mully hav \n",
            "\n",
            "[545.0461525917053 (2800 56%) 363.8252]\n",
            "Wher for the doing up and the said the back and when the right and be with our in in the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[548.8873083591461 (2820 56%) 368.2759]\n",
            "Wher a shadow and before they for the parch the \n",
            "\n",
            "into the streat \n",
            "fall see a great I that stole the l \n",
            "\n",
            "[552.6134307384491 (2840 56%) 334.2897]\n",
            "Whers with a star am the dired to the think and that the said and the \n",
            "sing the silent to feath and th \n",
            "\n",
            "[556.7299592494965 (2860 57%) 342.6618]\n",
            "Wher and with the Same the store was woulder long and it in the dark and the will store the ride and t \n",
            "\n",
            "[560.4686906337738 (2880 57%) 354.4814]\n",
            "Whed the paried and the \n",
            "whill be \n",
            "and to be ever stood \n",
            "and Sam the dorner a going and the dight he b \n",
            "\n",
            "[564.2457358837128 (2900 57%) 443.7263]\n",
            "Wh the looked Frodo the diep the \n",
            "long of the like a was have the will of the \n",
            "light as shall now be s \n",
            "\n",
            "[568.0739228725433 (2920 58%) 349.6023]\n",
            "Whing plack at the tree stall \n",
            "not the last were rounder and stard and see stood to set can on they sh \n",
            "\n",
            "[571.8226752281189 (2940 58%) 321.1180]\n",
            "Whing and \n",
            "the was a sise a say in the like the the dos and his streed and the \n",
            "said the Elves the wan \n",
            "\n",
            "[575.6439898014069 (2960 59%) 367.2303]\n",
            "Wher indow stone the silluld on the dark some to the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[579.4634523391724 (2980 59%) 339.4005]\n",
            "Whing \n",
            "shall of the brands do not the said beared that a stert, and the \n",
            "reas were be shill the crilen \n",
            "\n",
            "[583.2499234676361 (3000 60%) 394.8634]\n",
            "Wh the gardous and the mape and the like \n",
            "the far and the was the bear of the \n",
            "\n",
            "\n",
            "\n",
            "'was stood \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[587.0635912418365 (3020 60%) 330.7178]\n",
            "Whing have to the said Gonder \n",
            "should see stried the Gourntains wan to somely was not \n",
            "reast they slip \n",
            "\n",
            "[590.8613693714142 (3040 60%) 312.9934]\n",
            "Wher the \n",
            "see \n",
            "his ligf the end the \n",
            "the \n",
            "have \n",
            "the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[594.6600759029388 (3060 61%) 316.5977]\n",
            "Wher and \n",
            "year to the was and shall the seemed and shore out \n",
            "and whished and the \n",
            "stoll the fire of t \n",
            "\n",
            "[598.4522707462311 (3080 61%) 351.5942]\n",
            "Whany of the more his enem, and the more to he had be far be the looked what was beter from the parrin \n",
            "\n",
            "[602.2066719532013 (3100 62%) 320.9540]\n",
            "Wher is the pussed and his land the for the light of the tuin the shall came we said the \n",
            "lought of th \n",
            "\n",
            "[606.1351730823517 (3120 62%) 357.1681]\n",
            "Wh for the wemer to have woth would he was host until peases he do not we had from the \n",
            "his come in th \n",
            "\n",
            "[609.8636448383331 (3140 62%) 317.1667]\n",
            "Wher streat of the wind the \n",
            "diester the feep and the wind the Eld not stood, and the there well the r \n",
            "\n",
            "[613.6312143802643 (3160 63%) 327.1614]\n",
            "Whing the \n",
            "the shate the Eless be the canse the mist was reate in the wass the will be the was all the \n",
            "\n",
            "[617.4466364383698 (3180 63%) 365.1461]\n",
            "Whing the \n",
            "silent of the diern it some the Breat dark and the crear the Sament the went it can before  \n",
            "\n",
            "[621.5313806533813 (3200 64%) 342.9457]\n",
            "Wher and the said the been the light in the pooder a \n",
            "some put the \n",
            "intale \n",
            "the had \n",
            "and the good and  \n",
            "\n",
            "[625.4256207942963 (3220 64%) 325.1699]\n",
            "Whe they still to the would his spars of the reed where the far a dood, and see is light the know \n",
            "all \n",
            "\n",
            "[629.3162558078766 (3240 64%) 334.0274]\n",
            "Wher and they had begon about his chander as a light a can the still the was all the fains and the ree \n",
            "\n",
            "[633.4916059970856 (3260 65%) 307.1721]\n",
            "Wher the Sepper the \n",
            "\n",
            "\n",
            "The rest was had he \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Eard the stook to the Breat was four for \n",
            "\n",
            " \n",
            "\n",
            "[637.4548232555389 (3280 65%) 334.1458]\n",
            "Whe had not a land. 'We seet of the light \n",
            "aid the ring the prong the Mords of the was was a good and  \n",
            "\n",
            "[641.1888375282288 (3300 66%) 396.1420]\n",
            "Wher the mountll that the over Gander the mound the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Somer some to \n",
            "reat the Was for and that was  \n",
            "\n",
            "[644.9827270507812 (3320 66%) 359.5249]\n",
            "Wh the dark and had had some for he was to the tree to the down the come to the told see he seemed the \n",
            "\n",
            "[648.8281798362732 (3340 66%) 476.4279]\n",
            "Whing his shall for a where laid that is a ligull in the \n",
            "surt that his theed the beater the \n",
            "mend the \n",
            "\n",
            "[653.2442271709442 (3360 67%) 345.6022]\n",
            "Wher wish the canding will in the light. \n",
            "\n",
            "'Nor was see die great his not the \n",
            "said \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'Sam silen \n",
            "\n",
            "[657.0815055370331 (3380 67%) 323.3296]\n",
            "Whered of the \n",
            "sead that will he stell and they see the light will the feep the peamed the were will \n",
            " \n",
            "\n",
            "[661.0011084079742 (3400 68%) 362.2417]\n",
            "Wh was said some of the wander they are way in the must a strild be was be stried the Sam \n",
            "with the sh \n",
            "\n",
            "[664.7421028614044 (3420 68%) 352.7511]\n",
            "Whing of the hear in the ment of the fecked of the do not store the more of the right in the the slook \n",
            "\n",
            "[668.7420589923859 (3440 68%) 308.8052]\n",
            "When the last for the make the shadow of the real sonce, and the great bet the trate a see have see wa \n",
            "\n",
            "[672.6795952320099 (3460 69%) 324.1937]\n",
            "Whing to the men at the dark the took of the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "dow reat brows of Gan \n",
            "\n",
            "[676.4472742080688 (3480 69%) 302.0018]\n",
            "Wher the reat like upon the song the skir. \n",
            "\n",
            "'I son of the sound and the Cares and far in the dark and \n",
            "\n",
            "[680.1672720909119 (3500 70%) 336.1169]\n",
            "Wher the darrissed fear now the four; and that the door and had so \n",
            "sould could some and the dorst of  \n",
            "\n",
            "[683.9471569061279 (3520 70%) 332.1151]\n",
            "Wh in the dark and hel. It you he dead she can upon in the with \n",
            "that he shall the said this some of t \n",
            "\n",
            "[687.7019712924957 (3540 70%) 310.3564]\n",
            "Whers \n",
            "the River fel he still not \n",
            "the dodent that his hearry and \n",
            "shout of \n",
            "the \n",
            "readorn of the \n",
            "look \n",
            "\n",
            "[691.4625771045685 (3560 71%) 323.1350]\n",
            "Wher; and end upon in the \n",
            "shall the leatting the some. \n",
            "\n",
            "'Well and the come him see have and least an \n",
            "\n",
            "[695.5667042732239 (3580 71%) 313.6351]\n",
            "Wh parth it came and bechouther the great of the said Gonder the door and a \n",
            "shall of \n",
            "and stally of B \n",
            "\n",
            "[699.3865730762482 (3600 72%) 357.6278]\n",
            "Wher and as the master said the may in the broke the peave the road for the looked the long of \n",
            "come a \n",
            "\n",
            "[703.1967484951019 (3620 72%) 363.7906]\n",
            "Wher for the found the Ores of the of the \n",
            "not to \n",
            "left of the ridery the Whing with the more, and we  \n",
            "\n",
            "[707.2879180908203 (3640 72%) 330.9389]\n",
            "Wher a furted to the road had more and the end though the endain in the wind the \n",
            "not in the Dair upon \n",
            "\n",
            "[711.0703661441803 (3660 73%) 313.7627]\n",
            "Whey begents and the tree the Elfurt the flack and the made a \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[714.8036940097809 (3680 73%) 347.4867]\n",
            "Wher a stone of the did of the far that the dreat his last the mind and \n",
            "the \n",
            "ever the said Gandalf. ' \n",
            "\n",
            "[719.0363969802856 (3700 74%) 341.0305]\n",
            "Whing that \n",
            "see pild of the compon the bear. 'You said to the Elves of the gatreared to the stried of  \n",
            "\n",
            "[723.2827913761139 (3720 74%) 312.8754]\n",
            "Whe was of the long and dark the houd he wood and the or should be in the \n",
            "into he was was the far of  \n",
            "\n",
            "[727.077574968338 (3740 74%) 328.4830]\n",
            "Whan the was strouddel's \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[730.8375177383423 (3760 75%) 323.6900]\n",
            "Wher as from have the many in the meard some the pare of the land to the have \n",
            "great the maghing for t \n",
            "\n",
            "[734.7837133407593 (3780 75%) 343.8814]\n",
            "Wher \n",
            "him in the has feght at the gate, and we well he see the dark of the drester were still the pass \n",
            "\n",
            "[738.5614814758301 (3800 76%) 347.9936]\n",
            "Wher had as is the great great he come and the words of the \n",
            "see come his land of the Wight about the  \n",
            "\n",
            "[742.3199472427368 (3820 76%) 320.5456]\n",
            "Whis we small no come his \n",
            "come the look the Hour and the found the great of the \n",
            "said \n",
            "and came. The  \n",
            "\n",
            "[746.3689634799957 (3840 76%) 360.9940]\n",
            "Wher was and at long all the trees and should Gandalf they could been the tree that must and \n",
            "must of  \n",
            "\n",
            "[750.1824898719788 (3860 77%) 378.3432]\n",
            "Wher \n",
            "no despon the said. 'I was and the \n",
            "long be will sword to the got on the wander he and the \n",
            "all  \n",
            "\n",
            "[753.9653754234314 (3880 77%) 358.5754]\n",
            "Whing a lives of the said \n",
            "with had him to have \n",
            "with was for the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[757.7714595794678 (3900 78%) 281.9326]\n",
            "Wher that westered to as the ill to still on the passed the startel a gilen they and the steen and the \n",
            "\n",
            "[761.5113000869751 (3920 78%) 332.4417]\n",
            "Wher and his and the that he stalled of the \n",
            "chater and in the of the great reasing apon a gisthor. 'I \n",
            "\n",
            "[765.6468951702118 (3940 78%) 336.1776]\n",
            "Wher no and the Same for a flay of the like of the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[769.4345512390137 (3960 79%) 337.2774]\n",
            "Whing that and see the griess they soon \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[773.3961720466614 (3980 79%) 320.3604]\n",
            "Whing to \n",
            "the wind they stege came to he hear and kney with the gunted and there would his men and \n",
            "st \n",
            "\n",
            "[777.4512696266174 (4000 80%) 308.2335]\n",
            "Whing as the stone wan said the plade of the \n",
            "fall all and the ourse the stars and \n",
            "was hear rost to \n",
            " \n",
            "\n",
            "[781.1907217502594 (4020 80%) 340.1468]\n",
            "Whe of \n",
            "the iting of the lander to the out on the still to the warage to have had had like and soins o \n",
            "\n",
            "[785.1552634239197 (4040 80%) 310.2047]\n",
            "Wher \n",
            "but the dark of the threat of the Elves was \n",
            "the dalked of the ever for the pather and the Enet  \n",
            "\n",
            "[789.1042022705078 (4060 81%) 294.6498]\n",
            "Wher the \n",
            "\n",
            "Ere be space she great the dead of the \n",
            "was men and the streather any he word had wall for  \n",
            "\n",
            "[792.8506464958191 (4080 81%) 345.9707]\n",
            "Whan no a spead of the said, ' said Sam lay the \n",
            "said the \n",
            "and it me had last a feep many we said as a \n",
            "\n",
            "[796.6515831947327 (4100 82%) 347.7091]\n",
            "Whing the mounted to should be as the bragher. But I dark and he still the dised in the vingalf slowly \n",
            "\n",
            "[800.4369151592255 (4120 82%) 351.2022]\n",
            "Wher the treed. But the mised the green that his mester and should what we would be tright it all the  \n",
            "\n",
            "[804.2736887931824 (4140 82%) 356.3438]\n",
            "Whing the more of the will of the looked as the said that is felt had all the was the Had for the will \n",
            "\n",
            "[808.1677784919739 (4160 83%) 325.5389]\n",
            "Wholl the deam in the head they sate that my the spokent of the stare to the Elves any to the Fords an \n",
            "\n",
            "[811.9901647567749 (4180 83%) 390.0747]\n",
            "When of the preat the right the now the been of I stood sound the \n",
            "was in the was not coman that have  \n",
            "\n",
            "[816.03711104393 (4200 84%) 319.2088]\n",
            "Whing of do not shouth on the last of the itent as a sight a stread as the were not had said Frodo \n",
            "to \n",
            "\n",
            "[819.9713296890259 (4220 84%) 348.7609]\n",
            "Wher and the gatewalf of the was falls one and the should \n",
            "and like and fet the said it down the land  \n",
            "\n",
            "[823.9017679691315 (4240 84%) 327.7090]\n",
            "Whand a darkness alast and his loud and the far and was the \n",
            "his from the the and heard the hords \n",
            "pla \n",
            "\n",
            "[827.6583287715912 (4260 85%) 349.3180]\n",
            "When he long the topes of the wall the Carrien \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[831.6545717716217 (4280 85%) 345.2430]\n",
            "Wher the of the great have great he seen the seepraing and the Sam gore the long a came the stooned th \n",
            "\n",
            "[835.4633393287659 (4300 86%) 330.6897]\n",
            "Wher the said. 'But it bake \n",
            "his his and the passed the called the cleared of the mound the hold of th \n",
            "\n",
            "[839.2243440151215 (4320 86%) 296.0147]\n",
            "Whe that where away of the first one is now the was not know the hounds of the words of the \n",
            "said Gand \n",
            "\n",
            "[842.9666073322296 (4340 86%) 377.1146]\n",
            "Whouse in the Gallay. And the old had heared and the \n",
            "though and the \n",
            "Cast was see was they was good a \n",
            "\n",
            "[846.7793984413147 (4360 87%) 322.9765]\n",
            "When the Elven the great from the \n",
            "the there it great of the would he stally with the \n",
            "the reept we to \n",
            "\n",
            "[850.5366361141205 (4380 87%) 348.5705]\n",
            "Wher of it in the walls the roman we had more the \n",
            "are the winder up the words of the Elves of the Elv \n",
            "\n",
            "[854.2841861248016 (4400 88%) 307.8856]\n",
            "Whing to his even a will the all the more the which it the was the said. 'The partand the in the wondo \n",
            "\n",
            "[858.21737241745 (4420 88%) 317.8882]\n",
            "Whing of the dies of the shimbles she said. 'I have so to the \n",
            "little speared the looked the shall the \n",
            "\n",
            "[862.5412800312042 (4440 88%) 337.1955]\n",
            "Wher in the fing the \n",
            "seet was the gantain. No know we sours of the \n",
            "trough the many the vingur, shall \n",
            "\n",
            "[866.4959185123444 (4460 89%) 335.5805]\n",
            "Whelled he was have the rountains to the horse with \n",
            "the Mare of the gate to the mountain was saw he s \n",
            "\n",
            "[870.3162035942078 (4480 89%) 405.9601]\n",
            "Whing that is like a passed the Cander, and the \n",
            "store the \n",
            "was he was stranged, the chat his was a st \n",
            "\n",
            "[874.2362861633301 (4500 90%) 321.9654]\n",
            "Whing the roing of the still and the was \n",
            "the resionst at \n",
            "could reeding of \n",
            "the \n",
            "some of the end \n",
            "and \n",
            "\n",
            "[878.3217453956604 (4520 90%) 337.1613]\n",
            "Wher stread in the will be sight and the mountains of the \n",
            "with the peentain and shours of \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[882.3581008911133 (4540 90%) 338.4955]\n",
            "Whous of the would shell not the pattress of the \n",
            "spoke and send you will \n",
            "at the lands had like the h \n",
            "\n",
            "[886.3307659626007 (4560 91%) 314.1814]\n",
            "Whe trange of the \n",
            "over hin \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The in the dark of the felt he wall when the \n",
            "said \n",
            "\n",
            "[890.1425993442535 (4580 91%) 320.5251]\n",
            "Wher a feell the was a fatter \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "the said the Ring the did and the sunce of the gorther and like \n",
            "\n",
            "[894.1469094753265 (4600 92%) 316.5173]\n",
            "Whing that was some the other and see heard of palled \n",
            "to \n",
            "some and the brands and the Riff the said t \n",
            "\n",
            "[898.0884695053101 (4620 92%) 344.5665]\n",
            "Whing a \n",
            "gold \n",
            "sung the Called \n",
            "\n",
            "to be some of the end a far \n",
            "for a passed the dreat the soon on the m \n",
            "\n",
            "[901.8810386657715 (4640 92%) 354.1585]\n",
            "Whand be was strose and the \n",
            "site the were we said \n",
            "and what was begain and his bear and all the \n",
            "soun \n",
            "\n",
            "[905.7369604110718 (4660 93%) 288.8109]\n",
            "Whas \n",
            "could he been the crose the Hall the gladied it the meard and you not the night the dier \n",
            "at the \n",
            "\n",
            "[909.5060489177704 (4680 93%) 349.4806]\n",
            "Whe will a little all his place the Hight in the think a see could get the bring the many and had not  \n",
            "\n",
            "[913.3219330310822 (4700 94%) 310.2991]\n",
            "Whand of the will and the gone of the long of the mores of the Elves of the wing of the looked the \n",
            "we \n",
            "\n",
            "[917.1284666061401 (4720 94%) 318.9844]\n",
            "Whing that he war sight to the Elves was strange to the \n",
            "think to the back see far \n",
            "the with that he w \n",
            "\n",
            "[921.3841261863708 (4740 94%) 315.5910]\n",
            "Whous and had swars, \n",
            "and the great was belong of the Rides of he prot of his beccill for a sat the \n",
            "m \n",
            "\n",
            "[925.7599020004272 (4760 95%) 344.9148]\n",
            "Wher \n",
            "the \n",
            "rease the \n",
            "see we he \n",
            "doon, and the \n",
            "rears where and sign that we still to the great seemed \n",
            "\n",
            "[929.5824887752533 (4780 95%) 303.9991]\n",
            "Wher \n",
            "who heard the wassed the was shall the more of the dound were \n",
            "goat of that some in the was stro \n",
            "\n",
            "[933.6633408069611 (4800 96%) 342.6319]\n",
            "Wh, at a shall and the great seen the sone was his rearly have can had not to could near a shall of th \n",
            "\n",
            "[937.5347030162811 (4820 96%) 299.9017]\n",
            "Wher stars of the some the \n",
            "seam in \n",
            "the rider and \n",
            "brand. 'Well did and should do not song and the la \n",
            "\n",
            "[941.2996673583984 (4840 96%) 330.9485]\n",
            "Wher the Ring he are the last of did and many store the dissare and with the paces are off and land to \n",
            "\n",
            "[945.3270838260651 (4860 97%) 331.2636]\n",
            "Wher was were leader may feary, and un that hands of the looked they a come the Eld not the many the s \n",
            "\n",
            "[949.1823270320892 (4880 97%) 388.4365]\n",
            "Wher a had be \n",
            "singer in the seemed the \n",
            "are the Elves reached the wing the leak \n",
            "now, and the \n",
            "crose  \n",
            "\n",
            "[953.0097460746765 (4900 98%) 365.3812]\n",
            "Wher many to the \n",
            "sead to the trated by the fing and looked of Bardated and ever some had situred to h \n",
            "\n",
            "[956.7907907962799 (4920 98%) 329.7281]\n",
            "Wher that \n",
            "their had rould in the hone the gone the great he was all the lend great of the far his of  \n",
            "\n",
            "[960.5116987228394 (4940 98%) 292.3771]\n",
            "Wher and must \n",
            "there \n",
            "were stood was \n",
            "him a silent is a deshing \n",
            "when the grey could \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[964.3222930431366 (4960 99%) 350.1354]\n",
            "Wher far and a could see \n",
            "that is that was reat \n",
            "rear that \n",
            "was been that we have fire the wagh of the \n",
            "\n",
            "[968.0453307628632 (4980 99%) 314.9392]\n",
            "Whe with here can and the tree and be came the estare shaped him. No men for \n",
            "the rome to more the wit \n",
            "\n",
            "[971.7825374603271 (5000 100%) 306.1412]\n",
            "Whose the great good and that we as a some to the trees of the door with the stone the dither the eith \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# n_epochs = 2000\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())    \n",
        "  loss_avg += loss_\n",
        "  \n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "hHqRYv0nkL9V",
        "outputId": "2216da71-1def-4d3e-917e-1a2ae0fb4c78"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-87b9bdda3354>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'currentModel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"currentModel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Ee0so6aKJ5L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb368c1-1056-4e32-eb68-97fbffd8fd24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Th\n",
            " There was the same out they took the great came the \n",
            "things that the \n",
            "sound the last the dreat of the dark and have the riders be called on the \n",
            "beather that sound of the walk of the passed the Roots, t \n",
            "\n",
            " he\n",
            " he do not was that hell, and the hobbits of the end the Sam see better to the was that was a sunder of the Roother the said the drider to the now the now the soon the hight a that stranged but to do not \n",
            "\n",
            " ca\n",
            " can the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Chore far. And \n",
            "thought the fear and stalls and the sound that you have so shound the spoting a \n",
            "into the \n",
            "dear of the \n",
            "\n",
            " Th\n",
            " Through the still to looked the rook that was but a shadow that we said the near back \n",
            "and the wonder of the \n",
            "they that good they still to the sound a still that here and the fire the last or strouses t \n",
            "\n",
            " wh\n",
            " where to long in the word of the River of the right of the fing the \n",
            "door shast that be the might that the great the stood the she wear for the seemir to the round the pass and see the Root sound of the \n",
            "\n",
            " wh\n",
            " whith and the work of the \n",
            "was of the \n",
            "took to the \n",
            "water and the traing the wind the bees the sound in the \n",
            "that words of the strost the rold \n",
            "the \n",
            "ountulmer the our beft the sudden the troad to the \n",
            "r \n",
            "\n",
            " lo\n",
            " looked that the \n",
            "was ley. I do not the of the was that he said that he great was \n",
            "the tranger of Gollum trow rose to desters that for a strain and the rounder that little said the Countain thing here an \n",
            "\n",
            " ca\n",
            " can bear the marage the Rooth the was been the River his stood they the rear a they horse the space and the sings to the Elves to here we came the sight of the sking and the \n",
            "into the road. \n",
            "\n",
            "'It the wa \n",
            "\n",
            " Th\n",
            " The sudden and the right and the \n",
            "through that is a said the soon the \n",
            "away his bear bether the pords and sound the boots and behind the sunder the shall the wood that was the fire the River the path th \n",
            "\n",
            " I \n",
            " I they trins that the \n",
            "the hollow to the reach with a fort \n",
            "the Enet old be part of the some that many the \n",
            "under them the farth and seemed the learth that have not with coups the Rider the round to the \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t0B8P54q78Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJhgDc2IauPE"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Generate output on a different dataset\n",
        "\n",
        "---\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n",
        "\n",
        "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = unidecode.unidecode(open('/content/TrumpSpeaches.txt').read())\n",
        "file_len = len(file)\n"
      ],
      "metadata": {
        "id": "7i5882jWC_Yx"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 2500\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "\n",
        " \n",
        "#decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses2 = []\n",
        "loss_avg = 0"
      ],
      "metadata": {
        "id": "29j6m-rT7zZO"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_chunk()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "SfGj0RgOBJy_",
        "outputId": "21fcc850-e8f0-439d-9b8d-db726f3efdc1"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"e a look at the deal he's making with Iran. He makes that deal, Israel maybe won't exist very long. It's a disaster, and we have to protect Israel. But...\\nSo we need people -- I'm a free trader. But th\""
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n_epochs = 2000\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())    \n",
        "  loss_avg += loss_\n",
        "  \n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('HUGE', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5GYPWkW73pK",
        "outputId": "1f220c3e-58cc-4246-e567-ef50bad10750"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[205.04574537277222 (100 4%) 350.9297]\n",
            "HUGE BEMENE IT DINT THE DE BBE OU THE THE DAN THEYT I TTOE AVNE WE SO THE THE NOUUT THE EREE ALET THAT U \n",
            "\n",
            "[224.04309678077698 (200 8%) 375.5424]\n",
            "HUGE THE THE ORE THE THE GOT THE TO THE EERE OOU THE ANET THE TRE DHERS TO OUN THE THE THE ORE THE ANEE  \n",
            "\n",
            "[243.27290296554565 (300 12%) 280.8697]\n",
            "HUGE OUS THE OULE OOU won't I can the very the waring country very be the se the want to the can the wil \n",
            "\n",
            "[262.2316403388977 (400 16%) 467.0982]\n",
            "HUGE ING AIN THE BOUM GOU CORE ING UNG THAN GOE FOME OUT WE THO UNT ANG I TING ANG ARE HEND ING THAT TE  \n",
            "\n",
            "[281.3165030479431 (500 20%) 354.3181]\n",
            "HUGE TO BENT HAVE THE AN HENG THE LOY HE ERE SEAN AS LEN THE GEVENG TH THE THE HAVE THE FOUT TING IND WE \n",
            "\n",
            "[300.32251954078674 (600 24%) 305.9919]\n",
            "HUGE We loth of the don't Pend the reating to the some to Madd and I was and I think in that and the ric \n",
            "\n",
            "[319.25694704055786 (700 28%) 348.7767]\n",
            "HUGE I SOTHE SOREN OU THE WE HAN THE WE WE BE NDO PEON WE HANG THEN WE WE WE WOUR TRE HEERE GOUD HE AND  \n",
            "\n",
            "[338.0907542705536 (800 32%) 346.6943]\n",
            "HUGE OUN THE ANT I DOUT THE THE URERE SIRE WE YOU CAC I THE DOULE LELLE IG TED SAC AND TAIS AT I UAN THE \n",
            "\n",
            "[357.22445726394653 (900 36%) 367.2429]\n",
            "HUGE MESE WE HEALD PEORE GOIT THE THE GEAN THES THE. THER THE WE BELAN THE THE OF REAN SONT THE WE NE TH \n",
            "\n",
            "[376.66330194473267 (1000 40%) 322.0877]\n",
            "HUGE HANDE OUT HOULL THE IN THE HAVE HENY AND IN THE THE MEPORAS THE THE WE THE WHE WE SOUD THE HAND I T \n",
            "\n",
            "[395.4072687625885 (1100 44%) 374.7696]\n",
            "HUGE WE WANE THE HE ALLE KENG OF THE FERE ORERE REREY I THE WERERY THE OF THE TO GOS THE PEALL OF LOUD E \n",
            "\n",
            "[414.4891548156738 (1200 48%) 289.5614]\n",
            "HUGE PAND WE MAN THE NEK FOU SOULLY THE NON THEY PONT THAT OF YOU ING THE HAT UAT THEY IN WONG HANG MOT  \n",
            "\n",
            "[433.5684449672699 (1300 52%) 340.7043]\n",
            "HUGE SAND THE RISS CHE SACT I'T THE WE OR DOING OUT SAN TO OF A THE OUL THE THE PEY ON THE MALL THE SO O \n",
            "\n",
            "[452.7710635662079 (1400 56%) 394.7721]\n",
            "HUGE OUN AND THE MONE OF THE HAT HE OUN THE NED AND OUR POOR MESE THE POUNG OF OUS SUNG OF AND OUT HAC I \n",
            "\n",
            "[471.9278881549835 (1500 60%) 296.4520]\n",
            "HUGE FOUC WE HAVE EHANG TO HAVE ARE ONE THE THE STO ERENOTHE SO WE HAVE TO LEREN IT HEANER GOUT I NOUND  \n",
            "\n",
            "[490.7747597694397 (1600 64%) 302.1852]\n",
            "HUGE I SEOPLE DOT SE THE THE STEM THER THE SAVE THE GOT THE THE AND SE PENN THE ERE SO THE SAND INL OUT  \n",
            "\n",
            "[509.8387620449066 (1700 68%) 364.6740]\n",
            "HUGE WAIN AND OF WE'LM BE WEALE LEAND THE WEST THE WHAND THE DOING THE STON WAD TO WE HAVE THE OUS ANE A \n",
            "\n",
            "[529.2121846675873 (1800 72%) 298.2126]\n",
            "HUGE JULL YOUT WE HER WE WON THE WE BETIN CO WE DOULL WRETHING WE WE WER BUT WE WE LER IN IS NEAR IT. I  \n",
            "\n",
            "[548.129504442215 (1900 76%) 328.2558]\n",
            "HUGE THE SAND SAVE HAS I A WAC I GO HE HAVE AR SO SO SARE AND WE HIS OND WHAT YOU SAID IT I LAD WE HAVE  \n",
            "\n",
            "[567.0415360927582 (2000 80%) 299.7143]\n",
            "HUGE HE ARE IT WE WE REAST. AND I TINE HE TO GOR IS FOUR TE IT SO THER. OUT WERE THEING. THE TO FON TO S \n",
            "\n",
            "[586.1027944087982 (2100 84%) 255.4554]\n",
            "HUGE GOAT THER AVE I KEONE YOU GEACN DON THE GERE. THE SE LEED THE SING I GOT WE'A DO WE'RE THE WE LED T \n",
            "\n",
            "[605.6095280647278 (2200 88%) 297.7232]\n",
            "HUGE THE SAND SAT AND THEY AND SAND THE ARE IN I PERIGGTER THE BESTEY SAVE NOT THET THE VEVET REATE STAD \n",
            "\n",
            "[624.6232101917267 (2300 92%) 352.5893]\n",
            "HUGE WAR CAVE HE ACA WHE DOR SO STEN WE LOULL IT WEVE YOU GOR GEAT OUT THE GOR LEVER SAND IN AND AND THE \n",
            "\n",
            "[643.76127409935 (2400 96%) 334.7813]\n",
            "HUGE I ANT A WAY AND TO WE COUR AND SO IT WE'LL ANT CAS THAT STAT THE GORETH AND THES THE MARE TO STO WA \n",
            "\n",
            "[662.8033607006073 (2500 100%) 280.8874]\n",
            "HUGE THE SN THES THAT AND I LOT THAY THE SAT THAT I THE SO THE OF THE SAND HE WAT STER THE THE SAT HAVE  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Paragraph \n"
      ],
      "metadata": {
        "id": "NqhPQAjvANaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that The Model is ok at figure out words. Other that the model is pretty bad at actually forming any coherant sentances. Marcov chains do a much better job at forming possible sentacnce that reselmble images. The model does improve overtime but seems to bottom for the most part. "
      ],
      "metadata": {
        "id": "LJwFEKhLP2x6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "j8GvUjGu8IUu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "lab6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}